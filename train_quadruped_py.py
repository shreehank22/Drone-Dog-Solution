# -*- coding: utf-8 -*-
"""train_quadruped.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aT6tDU5GkFyCNTQbKI23qsCdloBGxZgu
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import torch
import os

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold
from stable_baselines3.common.monitor import Monitor

NUM_OBS = 44
NUM_ACT = 8
TS = 0.025
MAX_STEPS_PER_EPISODE = 400

def quadrupedResetFcn():
    initial_obs = np.random.rand(NUM_OBS).astype(np.float32)
    return initial_obs

class QuadrupedEnv(gym.Env):
    metadata = {'render_modes': ['human']}

    def __init__(self):
        super(QuadrupedEnv, self).__init__()

        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(NUM_OBS,), dtype=np.float32
        )

        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(NUM_ACT,), dtype=np.float32
        )

        self.current_step = 0
        print("Placeholder QuadrupedEnv initialized.")

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 0

        initial_obs = quadrupedResetFcn()

        info = {}
        return initial_obs, info

    def step(self, action):
        if self.current_step >= MAX_STEPS_PER_EPISODE:
            obs = np.zeros(NUM_OBS, dtype=np.float32)
            reward = 0.0
            terminated = False
            truncated = True
            info = {}
            return obs, reward, terminated, truncated, info

        self.current_step += 1

        obs = np.random.rand(NUM_OBS).astype(np.float32)
        reward = np.random.rand()

        terminated = False
        truncated = (self.current_step >= MAX_STEPS_PER_EPISODE)

        info = {}

        return obs, reward, terminated, truncated, info

    def render(self, mode='human'):
        pass

    def close(self):
        pass

def main():
    np.random.seed(0)
    torch.manual_seed(0)

    env = gym.make_vec(lambda: gym.wrappers.TimeLimit(QuadrupedEnv(), max_episode_steps=MAX_STEPS_PER_EPISODE))
    env = Monitor(env)

    batch_size = 256
    learning_rate = 1e-3

    policy_kwargs = dict(net_arch=[256, 256])

    n_steps = 2048
    n_epochs = 10

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        policy_kwargs=policy_kwargs,
        seed=0,
        verbose=1,
        tensorboard_log="./ppo_quadruped_tensorboard/"
    )

    total_timesteps = 10_000 * MAX_STEPS_PER_EPISODE

    stop_train_callback = StopTrainingOnRewardThreshold(
        reward_threshold=300,
        verbose=1
    )

    eval_env = gym.make_vec(lambda: gym.wrappers.TimeLimit(QuadrupedEnv(), max_episode_steps=MAX_STEPS_PER_EPISODE))
    eval_env = Monitor(eval_env)

    eval_callback = EvalCallback(
        eval_env,
        n_eval_episodes=5,
        eval_freq=25 * MAX_STEPS_PER_EPISODE,
        log_path="./logs/",
        best_model_save_path="./logs/best_model/",
        deterministic=True,
        render=False,
        callback_on_new_best=stop_train_callback
    )

    do_training = False
    model_save_path = "rlQuadrupedAgent_PPO.zip"

    if do_training:
        print(f"Starting training for {total_timesteps} timesteps...")
        model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            log_interval=10
        )

        model.save(model_save_path)
        print(f"Training complete. Model saved to {model_save_path}")

    else:
        print(f"Skipping training. Loading pretrained agent from {model_save_path}...")
        if os.path.exists(model_save_path):
            model = PPO.load(model_save_path, env=env)
            print("Model loaded successfully.")

            print("Running 1 episode with the loaded agent...")
            obs, _ = env.reset()
            for _ in range(MAX_STEPS_PER_EPISODE):
                action, _states = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                if terminated or truncated:
                    print("Episode finished.")
                    break
        else:
            print(f"Error: Model file not found at {model_save_path}")
            print("Set do_training = True to train a new model.")

if __name__ == "__main__":
    main()